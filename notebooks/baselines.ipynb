{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv1_test = load_svmlight_file(\"/mlodata1/jb/data/rcv1_test.binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = load_svmlight_file(\"/mlodata1/jb/data/epsilon_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def loss(clf, X, y, reg):\n",
    "    baseline_loss = np.sum(np.log(1 + np.exp(-y * (X @ clf.coef_.transpose()).squeeze()))) / X.shape[0]\n",
    "    baseline_loss += reg * np.sum(np.square(clf.coef_)) / 2\n",
    "    return baseline_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 159.71, NNZs: 42735, Bias: 0.000000, T: 677399, Avg. loss: 0.088822\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 155.40, NNZs: 42735, Bias: 0.000000, T: 1354798, Avg. loss: 0.071352\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 153.77, NNZs: 42735, Bias: 0.000000, T: 2032197, Avg. loss: 0.069948\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 153.39, NNZs: 42735, Bias: 0.000000, T: 2709596, Avg. loss: 0.069411\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 153.10, NNZs: 42735, Bias: 0.000000, T: 3386995, Avg. loss: 0.069020\n",
      "Total training time: 2.48 seconds.\n",
      "training took 2.7119717597961426s\n",
      "loss: 0.08491887159733663\n"
     ]
    }
   ],
   "source": [
    "X, y = rcv1_test\n",
    "clf = SGDClassifier(loss='log', penalty='l2', alpha=1. / X.shape[0], fit_intercept=False, verbose=True)\n",
    "start = time.time()\n",
    "clf.fit(X, y)\n",
    "elapsed = time.time() - start\n",
    "print('training took {}s'.format(elapsed))\n",
    "print('loss: {}'.format(loss(clf, X, y, 1. / X.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 145.72, NNZs: 2000, Bias: 0.000000, T: 400000, Avg. loss: 0.377656\n",
      "Total training time: 2.55 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 138.60, NNZs: 2000, Bias: 0.000000, T: 800000, Avg. loss: 0.273479\n",
      "Total training time: 5.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 136.81, NNZs: 2000, Bias: 0.000000, T: 1200000, Avg. loss: 0.268350\n",
      "Total training time: 7.51 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 135.68, NNZs: 2000, Bias: 0.000000, T: 1600000, Avg. loss: 0.266313\n",
      "Total training time: 9.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 135.15, NNZs: 2000, Bias: 0.000000, T: 2000000, Avg. loss: 0.264829\n",
      "Total training time: 12.45 seconds.\n",
      "training took 15.255411863327026s\n",
      "loss: 0.2832243544222945\n"
     ]
    }
   ],
   "source": [
    "X, y = epsilon\n",
    "clf = SGDClassifier(loss='log', penalty='l2', alpha=1. / X.shape[0], fit_intercept=False, verbose=True)\n",
    "start = time.time()\n",
    "clf.fit(X, y)\n",
    "elapsed = time.time() - start\n",
    "print('training took {}s'.format(elapsed))\n",
    "print('loss: {}'.format(loss(clf, X, y, 1. / X.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
